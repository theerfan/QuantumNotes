Simply defining a loss function based on a global observable (that is, observables measuring all qubits) leads to barren plateaus even for shallow circuits with sharp priors76, while local observables (those comparing quantum states at the single-qubit level) avoid this issue. The latter is due not to bad inductive biases but rather to the fact that comparing objects in exponentially large Hilbert spaces requires an exponential precision, as their overlap is usually exponentially small.

[This is because all supervised quantum models ARE kernel methods!!]